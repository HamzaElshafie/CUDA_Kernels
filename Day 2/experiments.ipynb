{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Addition Kernel\n",
        "--------"
      ],
      "metadata": {
        "id": "WUgO4Xw8wtwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csazn-slwumh",
        "outputId": "5e9eb1d5-cd52-4950-992f-14e0b70a7182"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 23 14:11:31 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   54C    P8             18W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA"
      ],
      "metadata": {
        "id": "-Dk9wLNH1yJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vector_addition.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <chrono>\n",
        "#include <cstdlib>\n",
        "#include <ctime>\n",
        "\n",
        "__global__ void vectorAdd(const float* A, const float* B, float* C, int N)\n",
        "{\n",
        "    // Element_id (i) = block_id * block_size + thread_id\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (i < N)\n",
        "    {\n",
        "        C[i] = A[i] + B[i]; // A[i] will translate to *(A + i)\n",
        "    }\n",
        "}\n",
        "\n",
        "void vectorAddCPU(const float* A, const float* B, float* C, int N)\n",
        "{\n",
        "    for (int i = 0; i < N; ++i)\n",
        "    {\n",
        "        C[i] = A[i] + B[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "void initialiseVectors(float* A, float* B, int N)\n",
        "{\n",
        "    srand(static_cast<unsigned int>(time(0)));\n",
        "\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        A[i] = static_cast<float>(rand()); // divide by RAND_MAX later if you want to normalise values\n",
        "        B[i] = static_cast<float>(rand());\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename Func>\n",
        "double measureExecutionTime(Func func)\n",
        "{\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "    func();\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double, std::milli> duration = end - start;\n",
        "    return duration.count();\n",
        "}\n",
        "\n",
        "bool compareResults(const float *A, const float *B, int N)\n",
        "{\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        if (fabs(A[i] - B[i]) > 1e-4)\n",
        "        {\n",
        "            std::cout << \"Mismatch at index \" << i << \": CPU=\" << A[i] << \" GPU=\" << B[i] << std::endl;\n",
        "            return false;\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int N = 1 << 25; // 1 million elements\n",
        "    size_t size = N * sizeof(float); // Memory size needed to store the vectors for addition\n",
        "\n",
        "    // Allocate memory on the host (CPU)\n",
        "    float* A_host = (float*)malloc(size); // malloc return a void pointer\n",
        "    float* B_host = (float*)malloc(size);\n",
        "    float* C_host_cpu = (float*)malloc(size);\n",
        "    float* C_host_gpu = (float*)malloc(size);\n",
        "\n",
        "    initialiseVectors(A_host, B_host, N);\n",
        "\n",
        "    // Measure CPU execution time for vector addition\n",
        "    double cpu_time = measureExecutionTime([&]()\n",
        "    {\n",
        "        vectorAddCPU(A_host, B_host, C_host_cpu, N);\n",
        "    });\n",
        "\n",
        "    std::cout << \"CPU execution time: \" << cpu_time << \"ms\" << '\\n';\n",
        "\n",
        "    // Allocate memory on the device (GPU)\n",
        "    float* A_device;\n",
        "    float* B_device;\n",
        "    float* C_device;\n",
        "\n",
        "    cudaMalloc((void**)&A_device, size);\n",
        "    cudaMalloc((void**)&B_device, size);\n",
        "    cudaMalloc((void**)&C_device, size);\n",
        "\n",
        "    cudaMemcpy(A_device, A_host, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(B_device, B_host, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threads_per_block = 256;\n",
        "    int blocks_per_grid = (N + threads_per_block - 1) / threads_per_block;\n",
        "    std::cout << \"Launching kernel with \" << blocks_per_grid << \" blocks of \"\n",
        "          << threads_per_block << \" threads.\" << '\\n';\n",
        "\n",
        "    double gpu_time = measureExecutionTime([&]()\n",
        "    {\n",
        "        vectorAdd<<<blocks_per_grid, threads_per_block>>>(A_device, B_device, C_device, N);\n",
        "        cudaDeviceSynchronize();\n",
        "    });\n",
        "\n",
        "    std::cout << \"GPU execution time: \" << gpu_time << \"ms\" << '\\n';\n",
        "\n",
        "    cudaMemcpy(C_host_gpu, C_device, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    bool success = compareResults(C_host_cpu, C_host_gpu, N);\n",
        "    std::cout << (success ? \"CPU and GPU results match!\" : \"Results mismatch!\");\n",
        "\n",
        "    cudaFree(A_device);\n",
        "    cudaFree(B_device);\n",
        "    cudaFree(C_device);\n",
        "\n",
        "    free(A_host);\n",
        "    free(B_host);\n",
        "    free(C_host_cpu);\n",
        "    free(C_host_gpu);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9zATojVtqWM",
        "outputId": "e74d3159-2b01-4c8a-94f9-6e441271143f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vector_addition.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HsLFUQRsd9z",
        "outputId": "972445b6-2252-4284-bd7c-c00de356bf38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU execution time: 166.674ms\n",
            "Launching kernel with 131072 blocks of 256 threads.\n",
            "GPU execution time: 1.6756ms\n",
            "CPU and GPU results match!"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_89 vector_addition.cu -o vector_addition\n",
        "!./vector_addition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Triton"
      ],
      "metadata": {
        "id": "tnt6zbVa1twP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import time\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)\n",
        "\n",
        "# torch.manual_seed(0)\n",
        "\n",
        "size = 1 << 25\n",
        "x = torch.rand(size, device=DEVICE)\n",
        "y = torch.rand(size, device=DEVICE)\n",
        "x_cpu = torch.rand(size, device=torch.device(\"cpu\"))\n",
        "y_cpu = torch.rand(size, device=torch.device(\"cpu\"))\n",
        "\n",
        "@triton.jit\n",
        "def add_kernel(x_ptr, y_ptr, output_ptr, num_elements, BLOCK_SIZE: tl.constexpr):\n",
        "  # Get program index\n",
        "  pid = tl.program_id(axis=0) # blockIdx.x\n",
        "  block_start = pid * BLOCK_SIZE # blockIdx.x * blockDim.x\n",
        "  # Generate the range of global indices this program is responsible for\n",
        "  offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
        "  mask = offsets < num_elements # Guard against out of bound invalid operations\n",
        "\n",
        "  # Load vectors from DRAM, masking out any extra elements in case the input is not a\n",
        "  # multiple of the block size.\n",
        "  x = tl.load(x_ptr + offsets, mask=mask)\n",
        "  y = tl.load(y_ptr + offsets, mask=mask)\n",
        "  output = x + y\n",
        "\n",
        "  # Write result back to DRAM\n",
        "  tl.store(output_ptr + offsets, output, mask=mask)\n",
        "\n",
        "def add(x: torch.tensor, y: torch.tensor):\n",
        "  output = torch.empty_like(x)\n",
        "  assert x.device == DEVICE and y.device == DEVICE and output.device == DEVICE\n",
        "\n",
        "  num_elements = output.numel()\n",
        "  grid = lambda meta: (triton.cdiv(num_elements, meta[\"BLOCK_SIZE\"]), )\n",
        "  add_kernel[grid](x, y, output, num_elements, BLOCK_SIZE=256)\n",
        "\n",
        "  return output\n"
      ],
      "metadata": {
        "id": "0OC3g7TstH6n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ada25e6-c068-4862-d585-dc2e44c03305"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Warmup and cache kernel\n",
        "_ = add(x, y)\n",
        "\n",
        "# Measure Triton execution time\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "output_triton = add(x, y)\n",
        "torch.cuda.synchronize()\n",
        "end = time.perf_counter()\n",
        "triton_time = (end - start) * 1000\n",
        "\n",
        "# Measure PyTorch GPU execution time\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "output_torch = x + y\n",
        "torch.cuda.synchronize()\n",
        "end = time.perf_counter()\n",
        "pytorch_time = (end - start) * 1000\n",
        "\n",
        "# Measure PyTorch CPU execution time\n",
        "start = time.perf_counter()\n",
        "output_torch_cpu = x_cpu + y_cpu\n",
        "end = time.perf_counter()\n",
        "pytorch_time_cpu = (end - start) * 1000\n",
        "\n",
        "print(f\"PyTorch CPU execution time: {pytorch_time_cpu:.5f}ms\")\n",
        "print(f\"PyTorch GPU execution time: {pytorch_time:.5f}ms\")\n",
        "print(f\"Triton  execution time: {triton_time:.5f}ms\")\n",
        "print(f'The maximum difference between torch and triton is '\n",
        "      f'{torch.max(torch.abs(output_torch - output_triton))}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j3tLWywUNz9",
        "outputId": "8c8f091b-663a-4f31-cebb-24af868d740b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch CPU execution time: 15.63703ms\n",
            "PyTorch GPU execution time: 1.85400ms\n",
            "Triton  execution time: 1.99292ms\n",
            "The maximum difference between torch and triton is 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dPvq2GEaW6Pa"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}
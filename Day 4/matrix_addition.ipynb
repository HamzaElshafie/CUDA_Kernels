{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Matrix Addition Kernel\n",
        "--------"
      ],
      "metadata": {
        "id": "WUgO4Xw8wtwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csazn-slwumh",
        "outputId": "06bc2444-67f2-4964-a565-eb8ce19580ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr 28 12:00:54 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   35C    P8             11W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA"
      ],
      "metadata": {
        "id": "-Dk9wLNH1yJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix_addition.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <ctime>\n",
        "#include <chrono>\n",
        "#include <cstdlib>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void matrixAdd(const float* A, const float* B, float* C, int rows, int columns)\n",
        "{\n",
        "    // Get thread row index\n",
        "    int row_index = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    // Get thread column index\n",
        "    int column_index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    // Check out of bounds\n",
        "    if (row_index < rows && column_index < columns)\n",
        "    {\n",
        "        int index = row_index * columns + column_index;\n",
        "        C[index] = A[index] + B[index];\n",
        "    }\n",
        "}\n",
        "\n",
        "void matrixAddCPU(const float* A, const float* B, float* C, int rows, int columns)\n",
        "{\n",
        "    for (int row_index = 0; row_index < rows; row_index++)\n",
        "    {\n",
        "        for (int column_index = 0; column_index < columns; column_index++)\n",
        "        {\n",
        "            int index = row_index * columns + column_index;\n",
        "            C[index] = A[index] + B[index];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void initialiseVectors(float *A, float *B, int N)\n",
        "{\n",
        "    srand(static_cast<unsigned int>(time(0)));\n",
        "\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        A[i] = static_cast<float>(rand());\n",
        "        B[i] = static_cast<float>(rand());\n",
        "    }\n",
        "}\n",
        "\n",
        "// Experiment with auto instead of template later\n",
        "template <typename Func>\n",
        "double measureExecutionTime(Func func)\n",
        "{\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "    func();\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double, std::milli> duration = end - start;\n",
        "    return duration.count();\n",
        "}\n",
        "\n",
        "bool compareResults(const float *A, const float *B, int N)\n",
        "{\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        if (fabs(A[i] - B[i]) > 1e-4)\n",
        "        {\n",
        "            std::cout << \"Mismatch at index \" << i << \": CPU=\" << A[i] << \" GPU=\" << B[i] << std::endl;\n",
        "            return false;\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int num_rows = 1 << 13;\n",
        "    int block_size_rows = 19;\n",
        "    int num_columns = 1 << 13;\n",
        "    int block_size_columns = 19;\n",
        "\n",
        "    size_t size = num_rows * num_columns * sizeof(float);\n",
        "\n",
        "    // Allocate memory on host\n",
        "    float* A_host = (float*)malloc(size);\n",
        "    float* B_host = (float*)malloc(size);\n",
        "    float* C_mat_cpu = (float*)malloc(size);\n",
        "    float* C_mat_gpu = (float*)malloc(size);\n",
        "\n",
        "    // Initialise matrix\n",
        "    initialiseVectors(A_host, B_host, num_rows * num_columns);\n",
        "\n",
        "    // Measure CPU execution time\n",
        "    double cpu_time = measureExecutionTime([&]()\n",
        "    {\n",
        "        matrixAddCPU(A_host, B_host, C_mat_cpu, num_rows, num_columns);\n",
        "    });\n",
        "    std::cout << \"CPU execution time: \" << cpu_time << \" ms\" << '\\n';\n",
        "\n",
        "    // Allocate memory on device\n",
        "    float* A_device;\n",
        "    float* B_device;\n",
        "    float* C_device;\n",
        "\n",
        "    cudaMalloc((void**)&A_device, size);\n",
        "    cudaMalloc((void**)&B_device, size);\n",
        "    cudaMalloc((void**)&C_device, size);\n",
        "\n",
        "    // Copy data from host to device\n",
        "    cudaMemcpy(A_device, A_host, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(B_device, B_host, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define grid\n",
        "    int num_blocks_rows = (num_rows + block_size_rows - 1) / block_size_rows;\n",
        "    int num_blocks_columns = (num_columns + block_size_columns - 1) / block_size_columns;\n",
        "\n",
        "    dim3 block(block_size_columns, block_size_rows, 1);\n",
        "    dim3 grid(num_blocks_columns, num_blocks_rows, 1);\n",
        "\n",
        "    // Measure GPU execution time\n",
        "    double gpu_time = measureExecutionTime([&]()\n",
        "    {\n",
        "        matrixAdd<<<grid, block>>>(A_device, B_device, C_device, num_rows, num_columns);\n",
        "        cudaDeviceSynchronize();\n",
        "    });\n",
        "    std::cout << \"GPU execution time: \" << gpu_time << \" ms\" << '\\n';\n",
        "\n",
        "    // Copy results from device to host\n",
        "    cudaMemcpy(C_mat_gpu, C_device, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    bool success = compareResults(C_mat_cpu, C_mat_gpu, num_rows * num_columns);\n",
        "    std::cout << (success ? \"CPU and GPU results match!\" : \"Results mismatch!\") << '\\n';\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(A_device);\n",
        "    cudaFree(B_device);\n",
        "    cudaFree(C_device);\n",
        "\n",
        "    // Free host memory\n",
        "    free(A_host);\n",
        "    free(B_host);\n",
        "    free(C_mat_cpu);\n",
        "    free(C_mat_gpu);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9zATojVtqWM",
        "outputId": "f61a05cc-f1e0-4e26-d1bd-60a573a14e5e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrix_addition.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HsLFUQRsd9z",
        "outputId": "aff7108c-d7d9-40f1-da17-ad4097f77f52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU execution time: 349.847 ms\n",
            "GPU execution time: 3.42599 ms\n",
            "CPU and GPU results match!\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_89 matrix_addition.cu -o matrix_addition\n",
        "!./matrix_addition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Triton"
      ],
      "metadata": {
        "id": "tnt6zbVa1twP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import time\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)\n",
        "\n",
        "# Initialise matrices\n",
        "size = 1 << 13\n",
        "x = torch.rand((size, size), device=DEVICE)\n",
        "y = torch.rand((size, size), device=DEVICE)\n",
        "x_cpu = torch.rand((size, size), device=\"cpu\")\n",
        "y_cpu = torch.rand((size, size), device=\"cpu\")\n",
        "\n",
        "@triton.jit\n",
        "def matrix_add_kernel(\n",
        "    x_ptr, y_ptr, output_ptr,\n",
        "    M, N, # num_rows, num_columns\n",
        "    stride_xm, stride_xn,\n",
        "    stride_ym, stride_yn,\n",
        "    stride_om, stride_on,\n",
        "    BLOCK_SIZE_M: tl.constexpr, # block_size_rows\n",
        "    BLOCK_SIZE_N: tl.constexpr # block_size_columns\n",
        "    ):\n",
        "\n",
        "  pid_m = tl.program_id(0) # x\n",
        "  pid_n = tl.program_id(1) # y\n",
        "\n",
        "  row_start = pid_m * BLOCK_SIZE_M\n",
        "  column_start = pid_n * BLOCK_SIZE_N\n",
        "\n",
        "  rows = row_start + tl.arange(0, BLOCK_SIZE_M)\n",
        "  columns = column_start + tl.arange(0, BLOCK_SIZE_N)\n",
        "\n",
        "  # Compute flat memory offsets for each (row, col) pair in the tile\n",
        "  # using broadcasting to generate the full 2D grid of indices\n",
        "  offsets_x = rows.expand_dims(1) * stride_xm + columns.expand_dims(0) * stride_xn\n",
        "  offsets_y = rows.expand_dims(1) * stride_ym + columns.expand_dims(0) * stride_yn\n",
        "  offsets_o = rows.expand_dims(1) * stride_om + columns.expand_dims(0) * stride_on\n",
        "\n",
        "  # Allow access to elements where both row and column indices are in bounds\n",
        "  mask = (rows.expand_dims(1) < M) & (columns.expand_dims(0) < N)\n",
        "\n",
        "  # Load elements of the tiles from DRAM, masking out out-of-bound elements with 0.0\n",
        "  x_tile = tl.load(x_ptr + offsets_x, mask=mask, other=0.0)\n",
        "  y_tile = tl.load(y_ptr + offsets_y, mask=mask, other=0.0)\n",
        "\n",
        "  output_tile = x_tile + y_tile\n",
        "\n",
        "  # Write result back to DRAM\n",
        "  tl.store(output_ptr + offsets_o, output_tile, mask=mask)\n",
        "\n",
        "\n",
        "def matrix_add(x, y):\n",
        "  output = torch.empty_like(x)\n",
        "\n",
        "  assert x.device == DEVICE and y.device == DEVICE and output.device == DEVICE, \"Tensors must be on CUDA\"\n",
        "  assert x.shape == y.shape and x.shape == output.shape, \"Tensors must have identical dimension\"\n",
        "\n",
        "  M, N = output.shape\n",
        "\n",
        "  stride_xm, stride_xn = x.stride()\n",
        "  stride_ym, stride_yn = y.stride()\n",
        "  stride_om, stride_on = output.stride()\n",
        "\n",
        "  grid = lambda meta: (triton.cdiv(M, meta[\"BLOCK_SIZE_M\"]),\n",
        "                       triton.cdiv(N, meta[\"BLOCK_SIZE_N\"]))\n",
        "\n",
        "  matrix_add_kernel[grid](\n",
        "      x, y, output,\n",
        "      M, N,\n",
        "      stride_xm, stride_xn,\n",
        "      stride_ym, stride_yn,\n",
        "      stride_om, stride_on,\n",
        "      BLOCK_SIZE_M=32,\n",
        "      BLOCK_SIZE_N=32\n",
        "      )\n",
        "\n",
        "  return output\n",
        "\n",
        "# Warm up and cache kernel\n",
        "_ = matrix_add(x, y)\n",
        "\n",
        "# Measure Triton execution time\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "output_triton = matrix_add(x, y)\n",
        "torch.cuda.synchronize()\n",
        "end = time.perf_counter()\n",
        "triton_time = (end - start) * 1000\n",
        "\n",
        "# Measure PyTorch GPU execution time\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "output_torch_gpu = x + y\n",
        "torch.cuda.synchronize()\n",
        "end = time.perf_counter()\n",
        "pytorch_gpu_time = (end - start) * 1000\n",
        "\n",
        "# Measure Pytorch CPU execution time\n",
        "start = time.perf_counter()\n",
        "output_torch_cpu = x_cpu + y_cpu\n",
        "end = time.perf_counter()\n",
        "pytorch_cpu_time = (end - start) * 1000\n",
        "\n",
        "# Check correctness\n",
        "max_diff = torch.max(torch.abs(output_triton - output_torch_gpu))\n",
        "assert torch.allclose(output_triton, output_torch_gpu, atol=1e-5), \"Mismatch with PyTorch!\"\n",
        "\n",
        "print(f\"Triton time:       {triton_time:.3f} ms\")\n",
        "print(f\"PyTorch GPU time:  {pytorch_gpu_time:.3f} ms\")\n",
        "print(f\"PyTorch CPU time:  {pytorch_cpu_time:.3f} ms\")\n",
        "print(f'The maximum difference between torch and triton is '\n",
        "      f'{torch.max(torch.abs(output_torch_gpu - output_triton))}')\n"
      ],
      "metadata": {
        "id": "0OC3g7TstH6n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429ab463-25fd-4520-d887-9e6f9316dfab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Triton time:       3.964 ms\n",
            "PyTorch GPU time:  3.548 ms\n",
            "PyTorch CPU time:  42.756 ms\n",
            "The maximum difference between torch and triton is 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VXOk_Q27d-BS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}